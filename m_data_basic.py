# ###########################################################################
#
# File    : m_data_basic.py
#
# Author  : Fernando MendonÃ§a (CIMA UAlg)
#
# Created : Mar. 2nd, 2023.
#
# Updated : Nov. 3rd, 2024.
#
# Descrp. : Contains the functions to convert data to BASIC web interface.
#
# ###########################################################################

from datetime import datetime
from glob import glob
from os import makedirs, path
from json import load
from shutil import copyfile, move, rmtree
from typing import Sequence

import xarray as xr

from m_data_sftp import SftpOps
from m_supp_xarray import buildtime
from m_supp_mohid import glueintime, convert2nc


# Tere should be a function here like the following:
def basictds(optype: int) -> int:
    # Check input file with TDS credentials
    # Run one of the operations: convnam, convsim
    # convnam takes netcdf and output dir
    # convsim takes input dir and output dir
    pass


def servinpts() -> tuple:
    jsonin = ".\\tdsinput.json"

    # The next test should be done in the main method for BASIC conversion,
    # which returns the error code 1.
    # if not path.isfile(jsonin):
    #     print(f"[ERROR] m_data_basic.servinpts: {jsonin} not found.")
    #     return "", "", ""
    
    with open(jsonin, "rb") as dat:
        inpts = load(dat)
        inpts: dict

    host = inpts.get("host", "")
    user = inpts.get("user", "")
    pswd = inpts.get("pswd", "")
    
    print("HOST :", host)
    print("USER :", user)
    print()

    return host, user, pswd


def convnam(fipt: str, outdir: str) -> None:
    """Specific function to convert data from NAM Central America,
    processed by SMS-Coastal, to the format read in the BASIC
    interface. After that, this data is sent to the interface TDS.

    Keyword arguments:
    - fipt: netCDF file generated by SMS-Coastal when processing NAM CA data;
    - outdir: path to output directory.
    """

    # NAM field attributes for BASIC:
    fldsattrs = {
        "PRES": {
            "var": "air_pressure_at_mean_sea_level",
            "long_name": "air pressure at mean sea level",
            "standard_name": "air_pressure_at_mean_sea_level",
            "valid_max": 110000, "valid_min": 85000,
            "units": "Pa", "coordinates": "lat lon",
        },
        "TMP": {
            "var": "air_temperature",
            "long_name": "air temperature",
            "standard_name": "air_temperature",
            "valid_max": 60, "valid_min": -90,
            "units": "degC", "coordinates": "lat lon",
        },
        "UGRD": {
            "var": "x_wind",
            "long_name": "x wind",
            "standard_name": "x_wind",
            "valid_max": 100, "valid_min": -100,
            "units": "m s-1", "coordinates": "lat lon",
        },
        "VGRD": {
            "var": "y_wind",
            "long_name": "y wind",
            "standard_name": "y_wind",
            "valid_max": 100, "valid_min": -100,
            "units": "m s-1", "coordinates": "lat lon",
        },
        "APCP": {
            "var": "precipitation",
            "long_name": "precipitation",
            "standard_name": "precipitation",
            "valid_max": 10, "valid_min": 0,
            "units": "kg m-2", "coordinates": "lat lon",
        },
        "RH": {
            "var": "relative_humidity",
            "long_name": "relative humidity",
            "standard_name": "relative_humidity",
            "valid_max": 1, "valid_min": 0,
            "units": "1", "coordinates": "lat lon",
        },
        "TCDC": {
            "var": "cloud_cover",
            "long_name": "cloud cover",
            "standard_name": "cloud_cover",
            "valid_max": 1, "valid_min": 0,
            "units": "1", "coordinates": "lat lon",
        },
    }
    
    print("NAM conversion to BASIC interface...")
    if path.isdir(outdir): rmtree(outdir)
    makedirs(outdir)
    
    # Open input file:
    dset = xr.open_dataset(fipt, use_cftime=True)
    data = dset["time"].data  # dates inside the netCDF.

    # Update dimensions attributes:
    val = "geographical coordinates, WGS84 projection"
    dset["latitude"].attrs["reference"] = val
    dset["longitude"].attrs["reference"] = val
    dset = dset.rename_dims({"latitude": "lat", "longitude": "lon"})
    dset = dset.rename_vars({"latitude": "lat", "longitude": "lon"})

    # Update variables inside dataset:
    for var in list(dset.data_vars):
        # Had to create a list with all variables in dset,
        # as in the next if statement they can be modified.

        # Remove non-convertable variables:
        if var not in fldsattrs.keys():
            dset = dset.drop_vars(var)
            continue

        print("", var, fldsattrs.get(var).get("var"))
        darr = dset[var]
        darr: xr.DataArray
        
        # Field attributes
        attrs = fldsattrs.get(var)
        attrs["maximum"] = float(darr.max())
        attrs["minimum"] = float(darr.min())

        # Update attributes and rename field:
        varid = attrs.pop("var")
        darr.rename(varid)
        darr.attrs = attrs

        dset.update({var: darr})
        dset = dset.rename_vars({var: varid})
    
    # Dates inside dataset:
    # ncdates = dset["time"].data.astype('datetime64[s]')
    # ncdates = ncdates.astype(datetime).tolist()
    # this is just to remember that I can convert datetime64[s] to datetime.
    
    # Number of datsets per day in NAM:
    ndsets = 8

    # Save one netCDF per day:
    print("Saving netCDFs...")
    for day in range(len(data)//ndsets):
        # output netCDF name:
        fout = data[day*ndsets].strftime("%Y%m%d%H.nc")       
        fout = path.join(outdir, fout)
        print("", path.basename(fout))
        
        # dataset output:
        xrout = dset.isel(time=slice(day*ndsets, (day+1)*ndsets))
        xrout.update({"time": buildtime(xrout["time"].data)})
        xrout.to_netcdf(fout) 

    dset.close()

    # Upload files to Thredds Data Server:
    sftp = SftpOps()
    host, user, pswd = servinpts()
    status = sftp.open(host, user, pswd)
    if status > 0: return

    srvdir = "/storage/thredds/METEO/NAM/NAM_0.12DEG_1L_3h/"
               
    ncs = glob(path.join(outdir, "*.nc"))
    inc = 0

    while status == 0 and inc < len(ncs):
        fout = srvdir + path.basename(ncs[inc])
        status = sftp.upfile(ncs[inc], fout)
        inc += 1

    sftp.close()


def convmodel(dailydirs: Sequence, tdsdir: str) -> int:
    """Makes the conversion of the data obtained from the BASIC operational
    chain, into the model interface format. The data is sent to the model's
    thredds server. The operation is done for each set of HDFs contained in
    the folders listed in 'dailydirs' and the outputs are saved locally in
    'tdsdir'.

    Keyword arguments:
    - dailydirs: list of database directories generated by SMS-Coastal;
    - tdsdir: path to a local output directory.
    """

    print("BASIC TDS Conversion Module")
    
    # Thredds folder:
    if path.isdir(tdsdir): rmtree(tdsdir)
    makedirs(tdsdir)

    # Check MOHID conversion file:
    fconv = ".\\outputs\\Convert2netcdf.dat"
    if not path.isfile(fconv):
        print("[ERROR] m_data_basic.convmodel:", end=" ")
        print("MOHID conversion file not found")
        return 1

    # Iterate daily directories:
    print("Merging files...")
    status = 0
    pos = 0
    
    while status == 0 and pos < len(dailydirs):
        # Get HDFs for a single day:
        print("", dailydirs[pos])
        hdfs = sorted(glob(dailydirs[pos] + "\\*.hdf5"))
        
        # Get HDFs date:
        dirdate = datetime.strptime(path.basename(dailydirs[pos]), "%y%m%d")
        fout = path.join(tdsdir, dirdate.strftime("%Y%m%d00.hdf5"))
        
        status = glueintime(".\\mohid\\merger", hdfs, fout)
        pos += 1

    if status > 0: return 1

    # Coversion to netCDF:
    print("Running conversion to netCDF...")
    hdfs = glob(tdsdir + "\\*.hdf5")
    pos = 0

    while status == 0 and pos < len(hdfs):
        hdf = hdfs[pos]
        print("", hdf, end=": ")

        copyfile(hdf, path.dirname(hdf) + "\\basic.hdf5")
        fout = path.dirname(hdf) + "\\basic.nc"
        status = convert2nc(".\\mohid\\convert2nc", fconv)

        # Loop control variable:
        pos += 1
        if status > 0: continue

        # Move output:
        move(fout, path.splitext(hdf)[0] + ".nc")

    if status > 0: return 1

    # Upload files to Thredds Data Server:
    print("Uploading to Thredds...")
    sftp = SftpOps()
    host, user, pswd = servinpts()
    status = sftp.open(host, user, pswd)
    if status > 0: return

    srvdir = "/storage/thredds/MOHID_WATER/CARTAGENA_100M_22L_1H/"
               
    ncs = glob(path.join(tdsdir, "*.nc"))
    inc = 0

    while status == 0 and inc < len(ncs):
        fout = srvdir + path.basename(ncs[inc])
        print("", fout)
        status = sftp.upfile(ncs[inc], fout)
        inc += 1

    sftp.close()
    return status
